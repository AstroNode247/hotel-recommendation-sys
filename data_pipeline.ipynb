{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aef4817",
   "metadata": {},
   "source": [
    "# Creation of ETL Pipeline using Pyspark\n",
    "Since the inital data is a text file, we need to create an etl pipeline that read it and load it to a database later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af47b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb7597a",
   "metadata": {},
   "source": [
    "# Create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740f3b0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpostgresql-42.2.26.jar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRecs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\software\\Anaconda3\\envs\\ml\\lib\\site-packages\\pyspark\\sql\\session.py:186\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    189\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32mE:\\software\\Anaconda3\\envs\\ml\\lib\\site-packages\\pyspark\\context.py:378\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mE:\\software\\Anaconda3\\envs\\ml\\lib\\site-packages\\pyspark\\context.py:133\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 133\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    136\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[1;32mE:\\software\\Anaconda3\\envs\\ml\\lib\\site-packages\\pyspark\\context.py:327\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 327\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mE:\\software\\Anaconda3\\envs\\ml\\lib\\site-packages\\pyspark\\java_gateway.py:102\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars\", \"postgresql-42.2.26.jar\") \\\n",
    "        .appName(\"Recs\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f4e5b",
   "metadata": {},
   "source": [
    "# Extract data\n",
    "First, we need to extract the data and further put it to pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "offering_dir = os.path.join('data', 'offering.txt')\n",
    "review_dir = os.path.join('data', 'review.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533c7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(review_dir) as f:\n",
    "    review = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1970bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a949681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "offering = spark.read.option(\"wholeFile\", True).option(\"mode\", \"PERMISSIVE\").json(offering_dir)\n",
    "review = spark.read.option(\"wholeFile\", True).option('mode', 'PERMISSIVE').json(review_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19df46ea",
   "metadata": {},
   "source": [
    "# Cleaning the data\n",
    "\n",
    "The first step is to drop all duplicates row in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "offering = offering.dropDuplicates()\n",
    "review = review.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f988475b",
   "metadata": {},
   "source": [
    "Let's see if the file contains nested json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e4c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "offering.printSchema()\n",
    "review.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d97fd9",
   "metadata": {},
   "source": [
    "As we can see in the schema above of review dataframe, there is a nested data. We need to flatten those nested data, to load it in a SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b5f3600",
   "metadata": {},
   "outputs": [],
   "source": [
    "offering = (offering\n",
    "            .withColumn('locality', fun.col('address.locality'))\n",
    "            .withColumn('postal_code', fun.col('address.postal-code'))\n",
    "            .withColumn('region', fun.col('address.region'))\n",
    "            .withColumn('street_address', fun.col('address.street-address'))\n",
    "            .drop(*['address'])\n",
    "           )\n",
    "review = (review\n",
    "            .withColumn('author_id', fun.col('author.id'))\n",
    "            .withColumn('author_num_cities', fun.col('author.num_cities'))\n",
    "            .withColumn('author_helpful_votes', fun.col('author.num_helpful_votes'))\n",
    "            .withColumn('author_num_reviews', fun.col('author.num_reviews'))\n",
    "            .withColumn('author_num_type_reviews', fun.col('author.num_type_reviews'))\n",
    "            .withColumn('author_username', fun.col('author.username'))\n",
    "            .withColumn('ratings_bussiness_service', fun.col('ratings.business_service_(e_g_internet_access)'))\n",
    "            .withColumn('ratings_check_in_front_desk', fun.col('ratings.check_in_front_desk'))\n",
    "            .withColumn('ratings_cleanliness', fun.col('ratings.cleanliness'))\n",
    "            .withColumn('ratings_location', fun.col('ratings.location'))\n",
    "            .withColumn('ratings_overall', fun.col('ratings.overall'))\n",
    "            .withColumn('ratings_rooms', fun.col('ratings.rooms'))\n",
    "            .withColumn('ratings_service', fun.col('ratings.service'))\n",
    "            .withColumn('ratings_sleep_quality', fun.col('ratings.sleep_quality'))\n",
    "            .withColumn('ratings_value', fun.col('ratings.value'))\n",
    "            .drop(*['author', 'ratings'])\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32533d04",
   "metadata": {},
   "source": [
    "The next step is to cast the date which is in string format to timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e581dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- date_stayed: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- num_helpful_votes: long (nullable = true)\n",
      " |-- offering_id: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- via_mobile: boolean (nullable = true)\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- author_num_cities: long (nullable = true)\n",
      " |-- author_helpful_votes: long (nullable = true)\n",
      " |-- author_num_reviews: long (nullable = true)\n",
      " |-- author_num_type_reviews: long (nullable = true)\n",
      " |-- author_username: string (nullable = true)\n",
      " |-- ratings_bussiness_service: double (nullable = true)\n",
      " |-- ratings_check_in_front_desk: double (nullable = true)\n",
      " |-- ratings_cleanliness: double (nullable = true)\n",
      " |-- ratings_location: double (nullable = true)\n",
      " |-- ratings_overall: double (nullable = true)\n",
      " |-- ratings_rooms: double (nullable = true)\n",
      " |-- ratings_service: double (nullable = true)\n",
      " |-- ratings_sleep_quality: double (nullable = true)\n",
      " |-- ratings_value: double (nullable = true)\n",
      " |-- period_stayed: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = (review.withColumn(\"date\", fun.to_date(\"date\", \"MMMM d, yyyy\"))\n",
    "         .withColumn(\"period_stayed\", fun.to_date(\"date_stayed\", \"MMMM yyyy\")))\n",
    "review.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f05761",
   "metadata": {},
   "source": [
    "Then encode `the author_id` to numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b2c895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = review.withColumn(\"author_num_id\", fun.dense_rank().over(Window.orderBy(\"author_id\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99afac4b",
   "metadata": {},
   "source": [
    "# Load to a database\n",
    "In order to exploit the data in the optimal way, we gonna load it to a Postgre database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24840cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving offering...\n",
      "Table offering already exist\n"
     ]
    }
   ],
   "source": [
    "print('Saving offering...')\n",
    "try :\n",
    "    offering.write.format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"offering\") \\\n",
    "        .option(\"user\", \"postgres\").option(\"password\", \"adm!@#\").save()\n",
    "    print(\"Save complete !\")\n",
    "except:\n",
    "    print(\"Table offering already exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f57637a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save complete !\n"
     ]
    }
   ],
   "source": [
    "review.write.mode(\"overwrite\").format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"review\") \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"adm!@#\").save()\n",
    "print(\"Save complete !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8d1349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f239d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
